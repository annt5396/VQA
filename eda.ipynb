{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def create_data_csv(p_dt, p_o, test=False):\n",
    "    with open(p_dt)  as d:\n",
    "        data = json.load(d)\n",
    "    print(data.keys())\n",
    "    \n",
    "    data_img = pd.DataFrame(data['images'])\n",
    "    print(data_img.shape)\n",
    "\n",
    "    data_dict_img = dict(zip(data_img['id'], data_img['filename']))\n",
    "\n",
    "    data_ano = pd.DataFrame(data['annotations'])\n",
    "    print(data_ano.shape)\n",
    "    data_ano['image'] = data_ano['image_id'].map(lambda x: data_dict_img[x])\n",
    "    \n",
    "    answer_space = list(data_ano['answer'].unique())\n",
    "#     answer_space = list(data_ano['answer'].value_counts()[data_ano['answer'].value_counts()>1].keys())\n",
    "    print(answer_space.index(answer_space[0]), answer_space[0])\n",
    "    # data_ano['label'] = data_ano['answer'].map(lambda x: answer_space.index(x))\n",
    "    if test:\n",
    "        data_ano['answer'] = data_ano['answer'].map(lambda x: '1')\n",
    "    else:\n",
    "        data_ano['answer'] = data_ano['answer'].map(lambda x: [x])\n",
    "        data_ano['dataset'] = data_ano['answer'].map(lambda x: 'vqa')\n",
    "        data_ano['question_id'] = data_ano['id']\n",
    "    data_ano.drop(['id', 'image_id'], axis=1).to_json(p_o, orient=\"records\")\n",
    "    \n",
    "    return answer_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['images', 'annotations'])\n",
      "(3763, 2)\n",
      "(23785, 4)\n",
      "0 the girl wearing glasses wears a red shirt\n"
     ]
    }
   ],
   "source": [
    "p_dt = '/home/annt/vqa/BLIP/dataset/evjqa/train-data/evjvqa_train.json'\n",
    "p_o = './annotation/train.json'\n",
    "answer_space = create_data_csv(p_dt, p_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/annt/miniconda3/envs/vqa/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from data import create_dataset, create_sampler, create_loader\n",
    "from data.vqa_dataset import vqa_collate_fn\n",
    "import yaml\n",
    "\n",
    "config = yaml.load(open('./configs/vqa.yaml', 'r'), Loader=yaml.Loader)\n",
    "\n",
    "datasets = create_dataset('vqa', config) \n",
    "samplers = [None, None]\n",
    "\n",
    "train_loader, test_loader = create_loader(datasets,samplers,\n",
    "                                              batch_size=[config['batch_size_train'],config['batch_size_test']],\n",
    "                                              num_workers=[4,4],is_trains=[True, False], \n",
    "                                              collate_fns=[vqa_collate_fn,None]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[[-1.6609, -1.6901, -1.6901,  ..., -1.5295, -1.4127, -1.4565],\n",
      "          [-1.3835, -1.6901, -1.7047,  ..., -1.3397, -0.6974, -1.1499],\n",
      "          [-1.1207, -1.6755, -1.6901,  ..., -1.0769, -0.3324, -0.5806],\n",
      "          ...,\n",
      "          [ 0.0909,  0.0909,  0.0909,  ...,  0.7625,  0.6603,  0.5581],\n",
      "          [ 0.0909,  0.0909,  0.0909,  ...,  0.4121,  0.3245,  0.1931],\n",
      "          [ 0.0909,  0.0909,  0.0909,  ..., -0.0842, -0.2302, -0.3178]],\n",
      "\n",
      "         [[-1.6020, -1.6320, -1.6320,  ..., -1.4820, -1.4369, -1.4669],\n",
      "          [-1.3169, -1.6320, -1.6470,  ..., -1.2568, -0.6415, -1.1218],\n",
      "          [-1.0467, -1.6170, -1.6320,  ..., -0.9417, -0.2063, -0.4764],\n",
      "          ...,\n",
      "          [ 0.1839,  0.1839,  0.1839,  ...,  0.2589,  0.1689,  0.0638],\n",
      "          [ 0.1839,  0.1839,  0.1839,  ..., -0.0712, -0.1763, -0.2963],\n",
      "          [ 0.1839,  0.1839,  0.1839,  ..., -0.4764, -0.5365, -0.6115]],\n",
      "\n",
      "         [[-1.2811, -1.3096, -1.3096,  ..., -1.1532, -1.0394, -1.0678],\n",
      "          [-1.0110, -1.3096, -1.3238,  ..., -0.9399, -0.3284, -0.7692],\n",
      "          [-0.7692, -1.2954, -1.3096,  ..., -0.6555,  0.0271, -0.2289],\n",
      "          ...,\n",
      "          [ 0.3399,  0.3399,  0.3399,  ..., -0.2431, -0.3568, -0.4279],\n",
      "          [ 0.3399,  0.3399,  0.3399,  ..., -0.4706, -0.5701, -0.6839],\n",
      "          [ 0.3399,  0.3399,  0.3399,  ..., -0.8261, -0.9114, -0.9541]]],\n",
      "\n",
      "\n",
      "        [[[ 1.9303,  1.9303,  1.9303,  ...,  1.9303,  1.9303,  1.9303],\n",
      "          [ 1.9303,  1.9303,  1.9303,  ...,  1.9303,  1.9303,  1.9303],\n",
      "          [ 1.9303,  1.9303,  1.9303,  ...,  1.9303,  1.9303,  1.9303],\n",
      "          ...,\n",
      "          [ 1.7844,  1.8865,  1.9157,  ...,  1.9303,  1.8573,  1.8135],\n",
      "          [ 1.7844,  1.8573,  1.9303,  ...,  1.9303,  1.9303,  1.9157],\n",
      "          [ 1.7844,  1.8573,  1.8865,  ...,  1.8573,  1.8573,  1.8281]],\n",
      "\n",
      "         [[ 2.0749,  2.0749,  2.0749,  ...,  2.0749,  2.0749,  2.0749],\n",
      "          [ 2.0749,  2.0749,  2.0749,  ...,  2.0749,  2.0749,  2.0749],\n",
      "          [ 2.0749,  2.0749,  2.0749,  ...,  2.0749,  2.0749,  2.0749],\n",
      "          ...,\n",
      "          [ 2.0749,  2.0449,  1.9998,  ...,  2.0149,  1.9398,  1.8948],\n",
      "          [ 2.0749,  2.0449,  2.0299,  ...,  2.0749,  2.0749,  2.0749],\n",
      "          [ 2.0749,  2.0449,  1.9848,  ...,  2.0749,  2.0749,  2.0299]],\n",
      "\n",
      "         [[ 2.1459,  2.1459,  2.1459,  ...,  2.1459,  2.1459,  2.1459],\n",
      "          [ 2.1459,  2.1459,  2.1459,  ...,  2.1459,  2.1459,  2.1459],\n",
      "          [ 2.1459,  2.1459,  2.1459,  ...,  2.1459,  2.1459,  2.1459],\n",
      "          ...,\n",
      "          [ 1.9753,  2.1032,  2.1317,  ...,  2.0179,  1.9468,  1.9042],\n",
      "          [ 1.9184,  2.0321,  2.1459,  ...,  2.1175,  2.1175,  2.1032],\n",
      "          [ 1.8757,  1.9895,  2.0890,  ...,  2.1459,  2.1317,  2.1032]]],\n",
      "\n",
      "\n",
      "        [[[-0.1718,  0.0325,  0.0909,  ...,  0.3829,  0.0325, -0.0405],\n",
      "          [ 0.0471, -0.1134, -0.2594,  ...,  0.1639, -0.0259, -0.0405],\n",
      "          [ 0.3099,  0.0033, -0.0988,  ...,  0.0471, -0.0550, -0.0259],\n",
      "          ...,\n",
      "          [ 0.0763,  0.0909,  0.1785,  ...,  0.3099,  0.2807,  0.1201],\n",
      "          [ 0.5873,  0.6603,  0.6749,  ...,  0.1931,  0.0471, -0.0550],\n",
      "          [ 0.8063,  0.8792,  0.8063,  ...,  0.3975,  0.2369,  0.0471]],\n",
      "\n",
      "         [[-0.1613,  0.0638, -0.0112,  ...,  0.5141,  0.2589,  0.2890],\n",
      "          [ 0.0638, -0.1012, -0.3114,  ...,  0.3940,  0.3040,  0.3940],\n",
      "          [ 0.3340,  0.0188, -0.1613,  ...,  0.2890,  0.2589,  0.4090],\n",
      "          ...,\n",
      "          [ 0.0638,  0.0939,  0.0789,  ...,  0.2740,  0.2589,  0.1089],\n",
      "          [ 0.5291,  0.6041,  0.5591,  ...,  0.1689,  0.0638,  0.0038],\n",
      "          [ 0.6642,  0.7542,  0.6792,  ...,  0.4090,  0.3040,  0.1839]],\n",
      "\n",
      "         [[ 0.0413,  0.2404,  0.2120,  ..., -0.4848, -0.8261, -0.9114],\n",
      "          [ 0.3115,  0.1551, -0.0867,  ..., -0.5844, -0.8972, -1.0394],\n",
      "          [ 0.5817,  0.2831,  0.0840,  ..., -0.6981, -0.9114, -0.9683],\n",
      "          ...,\n",
      "          [ 0.1551,  0.1693,  0.2973,  ...,  0.3399,  0.3257,  0.1977],\n",
      "          [ 0.6244,  0.6955,  0.7239,  ...,  0.1977,  0.1124,  0.0555],\n",
      "          [ 0.8092,  0.8945,  0.7950,  ...,  0.3684,  0.2831,  0.1693]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.7120, -0.6244, -0.2594,  ...,  0.0325, -0.1280, -0.3762],\n",
      "          [-0.5514, -0.3616, -0.0842,  ...,  0.0617, -0.0842, -0.3324],\n",
      "          [-0.5076, -0.4930,  0.3537,  ..., -0.3908, -0.3032, -0.3908],\n",
      "          ...,\n",
      "          [ 0.6895,  1.0106,  0.7771,  ..., -0.2010,  0.0617,  0.6019],\n",
      "          [ 0.6311,  0.9960,  1.0982,  ..., -0.3032,  0.1493,  0.7625],\n",
      "          [ 0.5435,  0.9084,  1.1858,  ...,  0.3099,  0.6603,  0.6311]],\n",
      "\n",
      "         [[ 0.1089,  0.1689,  0.4540,  ...,  0.0939, -0.2513, -0.7316],\n",
      "          [ 0.2289,  0.3040,  0.4240,  ...,  0.2139, -0.0862, -0.5215],\n",
      "          [ 0.2589,  0.1839,  0.8743,  ..., -0.1163, -0.1463, -0.4314],\n",
      "          ...,\n",
      "          [ 1.0694,  1.3845,  1.0694,  ..., -0.4614, -0.2063,  0.4240],\n",
      "          [ 0.9343,  1.3395,  1.4295,  ..., -0.5215, -0.0712,  0.6341],\n",
      "          [ 0.7992,  1.2344,  1.5496,  ...,  0.1539,  0.5141,  0.5291]],\n",
      "\n",
      "         [[-0.9541, -0.9114, -0.6128,  ..., -1.1247, -1.2669, -1.4518],\n",
      "          [-0.9541, -0.9683, -0.8261,  ..., -0.9967, -1.1674, -1.3807],\n",
      "          [-1.1389, -1.3380, -0.7123,  ..., -1.1816, -1.1389, -1.2527],\n",
      "          ...,\n",
      "          [-0.8119, -0.4137, -0.5133,  ..., -1.3807, -1.2527, -0.8261],\n",
      "          [-0.8403, -0.4279, -0.2004,  ..., -1.4376, -1.1247, -0.5986],\n",
      "          [-0.9256, -0.5275, -0.1151,  ..., -0.8403, -0.5701, -0.6839]]],\n",
      "\n",
      "\n",
      "        [[[-1.1499, -0.3324,  0.0763,  ..., -0.2448, -0.2156,  0.1201],\n",
      "          [-1.0185, -0.9164,  0.1931,  ..., -0.3178, -0.3178,  0.1055],\n",
      "          [-0.6390, -1.0185,  0.3391,  ..., -0.3178, -0.3616, -0.0113],\n",
      "          ...,\n",
      "          [-0.9893, -0.9456, -0.8434,  ...,  0.2515,  0.2077,  0.2223],\n",
      "          [-0.9893, -0.9748, -0.9456,  ...,  0.2515,  0.2077,  0.2077],\n",
      "          [-0.9748, -1.0039, -1.0039,  ...,  0.2661,  0.2077,  0.1931]],\n",
      "\n",
      "         [[-0.9867, -0.1613,  0.2439,  ...,  0.4691,  0.3940,  0.7092],\n",
      "          [-0.8516, -0.7616,  0.3790,  ...,  0.3940,  0.2890,  0.6942],\n",
      "          [-0.4614, -0.8666,  0.5591,  ...,  0.3940,  0.2439,  0.5741],\n",
      "          ...,\n",
      "          [-0.9867, -0.9567, -0.8516,  ..., -0.3564, -0.4014, -0.4014],\n",
      "          [-0.9867, -1.0017, -0.9567,  ..., -0.3564, -0.4014, -0.4014],\n",
      "          [-0.9717, -1.0167, -1.0467,  ..., -0.3414, -0.4014, -0.3864]],\n",
      "\n",
      "         [[-0.8545, -0.1578,  0.0840,  ...,  1.1647,  1.0936,  1.3496],\n",
      "          [-0.7266, -0.7408,  0.2120,  ...,  1.0794,  0.9656,  1.3354],\n",
      "          [-0.3568, -0.8261,  0.3826,  ...,  1.0936,  0.9230,  1.2074],\n",
      "          ...,\n",
      "          [-0.2857, -0.3142, -0.3000,  ..., -0.2431, -0.2857, -0.2573],\n",
      "          [-0.2715, -0.3284, -0.3711,  ..., -0.2431, -0.2857, -0.2573],\n",
      "          [-0.2431, -0.3568, -0.4279,  ..., -0.2289, -0.2857, -0.2573]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0763,  0.0763,  0.0763,  ...,  0.0763,  0.0763,  0.0763],\n",
      "          [ 0.0763,  0.0763,  0.0763,  ...,  0.0763,  0.0763,  0.0763],\n",
      "          [ 0.0763,  0.0763,  0.0763,  ...,  0.0763,  0.0763,  0.0763],\n",
      "          ...,\n",
      "          [ 0.0763,  0.0763,  0.0763,  ...,  0.0763,  0.0763,  0.0763],\n",
      "          [ 0.0763,  0.0763,  0.0763,  ...,  0.0763,  0.0763,  0.0763],\n",
      "          [ 0.0763,  0.0763,  0.0763,  ...,  0.0763,  0.0763,  0.0763]],\n",
      "\n",
      "         [[ 0.1689,  0.1689,  0.1689,  ...,  0.1689,  0.1689,  0.1689],\n",
      "          [ 0.1689,  0.1689,  0.1689,  ...,  0.1689,  0.1689,  0.1689],\n",
      "          [ 0.1689,  0.1689,  0.1689,  ...,  0.1689,  0.1689,  0.1689],\n",
      "          ...,\n",
      "          [ 0.1689,  0.1689,  0.1689,  ...,  0.1689,  0.1689,  0.1689],\n",
      "          [ 0.1689,  0.1689,  0.1689,  ...,  0.1689,  0.1689,  0.1689],\n",
      "          [ 0.1689,  0.1689,  0.1689,  ...,  0.1689,  0.1689,  0.1689]],\n",
      "\n",
      "         [[ 0.3399,  0.3399,  0.3399,  ...,  0.3399,  0.3399,  0.3399],\n",
      "          [ 0.3399,  0.3399,  0.3399,  ...,  0.3399,  0.3399,  0.3399],\n",
      "          [ 0.3399,  0.3399,  0.3399,  ...,  0.3399,  0.3399,  0.3399],\n",
      "          ...,\n",
      "          [ 0.3399,  0.3399,  0.3399,  ...,  0.3399,  0.3399,  0.3399],\n",
      "          [ 0.3399,  0.3399,  0.3399,  ...,  0.3399,  0.3399,  0.3399],\n",
      "          [ 0.3399,  0.3399,  0.3399,  ...,  0.3399,  0.3399,  0.3399]]]]), ['gian phòng này bày trí những vật gì?', '女性はどちらの手でバスケットを持っていますか?', '家はどんな素材で覆われていますか?', 'what is the man in the self-defense militia uniform holding in his hand?', 'người đàn ông đứng bên cạnh thiết bị gì?', 'what animal is the girl in black holding?', 'có bao nhiêu chiếc xe ô tô đang đậu ở trước cửa nhà?', 'người phụ nữ mặc áo đỏ dẫn theo bao nhiêu đứa bé?', 'nội dung trên hai tấm bảng đang phát sáng là gì?', '自転車に乗っている人は何人ですか?', 'biểu tượng của công ty là gì?', 'có bao nhiêu người đứng cạnh chiếc xe phía trước hẻm?', 'bạn nữ mặc áo đen đang làm gì?', 'how many girls stand here in the orchard?', 'chiếc giỏ người phụ nữ trong quán cầm có màu gì?', 'chiếc xuồng đi sau đang chở bao nhiêu người?'], ['những mô hình máy bay', 'バスケットを両手で持っています', '葉で覆われています。', 'he is holding a baton', 'người đàn ông đứng bên cạnh cái micro', 'she is holding an octopus.', 'một chiếc', 'hai đứa', 'thực đơn các món ăn trong một nhà hàng', '誰かが自転車に乗っています', 'biểu tượng của công ty là biểu tượng người bác sĩ', 'có một người đứng cạnh chiếc xe', 'đang chụp hình với bạn nữ đứng cạnh bên', 'two', 'chiếc giỏ có màu xanh', 'bốn người'], tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "for k in train_loader:\n",
    "    print(k)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 3, 480, 480])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41, 41, 41)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(k[2]), len(k[3]), sum(k[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16,\n",
       " ['what color is the pillow on the right?',\n",
       "  'what is the female holding in her right hand?',\n",
       "  'does there appear to be more spectators or more participants?',\n",
       "  'the tiles are in what shape?',\n",
       "  'what seafood is in the bowl?',\n",
       "  'what fabric do the shoes in the picture appear to be made of?',\n",
       "  'what is the number of the train?',\n",
       "  'what is the animal seen on the shirt?',\n",
       "  'how many forks?',\n",
       "  'is the man alone?',\n",
       "  'is he jumping?',\n",
       "  'where are they eating?',\n",
       "  'is there traffic on this road?',\n",
       "  'is that a live human child leaning on the rear bicycle tire?',\n",
       "  \"what's the number on the batter's back?\",\n",
       "  'what color is the ball?'])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(k[1]), k[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mval2014\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ls ./dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mannotation\u001b[0m/         eda.ipynb                requirements.txt\n",
      "BLIP.gif            eval_nocaps.py           SECURITY.md\n",
      "CODE_OF_CONDUCT.md  eval_retrieval_video.py  train_caption.py\n",
      "CODEOWNERS          LICENSE.txt              train_nlvr.py\n",
      "cog.yaml            \u001b[01;34mmodels\u001b[0m/                  train_retrieval.py\n",
      "\u001b[01;34mconfigs\u001b[0m/            predict.py               train_vqa.py\n",
      "\u001b[01;34mdata\u001b[0m/               pretrain.py              \u001b[01;34mtransform\u001b[0m/\n",
      "\u001b[01;34mdataset\u001b[0m/            \u001b[01;34m__pycache__\u001b[0m/             utils.py\n",
      "demo.ipynb          README.md\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mannotation\u001b[0m/         eda.ipynb                requirements.txt\n",
      "BLIP.gif            eval_nocaps.py           SECURITY.md\n",
      "CODE_OF_CONDUCT.md  eval_retrieval_video.py  train_caption.py\n",
      "CODEOWNERS          LICENSE.txt              train_nlvr.py\n",
      "cog.yaml            \u001b[01;34mmodels\u001b[0m/                  train_retrieval.py\n",
      "\u001b[01;34mconfigs\u001b[0m/            predict.py               train_vqa.py\n",
      "\u001b[01;34mdata\u001b[0m/               pretrain.py              \u001b[01;34mtransform\u001b[0m/\n",
      "\u001b[01;34mdataset\u001b[0m/            \u001b[01;34m__pycache__\u001b[0m/             utils.py\n",
      "demo.ipynb          README.md\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/annt/vqa/BLIP/dataset\n"
     ]
    }
   ],
   "source": [
    "cd dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install opendatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
      "Your Kaggle username:Your Kaggle Key:Downloading evjqa.zip to ./evjqa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 611M/611M [00:11<00:00, 56.1MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import opendatasets as od\n",
    "\n",
    "url_vqa = 'https://www.kaggle.com/datasets/haschool/evjqa'\n",
    "od.download(url_vqa)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mevjqa\u001b[0m/  \u001b[01;34mval2014\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evjvqa_train.json  \u001b[0m\u001b[01;34mtrain-images\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ls  evjqa/train-data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████| 2.56k/2.56k [00:00<00:00, 1.43MB/s]\n",
      "No config specified, defaulting to: cats-image/image\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset cats-image/image to /home/annt/.cache/huggingface/datasets/huggingface___cats-image/image/1.9.0/68fbc793fb10cd165e490867f5d61fa366086ea40c73e549a020103dcb4f597e...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 173k/173k [00:00<00:00, 245kB/s] \n",
      "Downloading data files: 100%|██████████| 1/1 [00:02<00:00,  2.65s/it]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 410.48it/s]\n",
      "                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset cats-image downloaded and prepared to /home/annt/.cache/huggingface/datasets/huggingface___cats-image/image/1.9.0/68fbc793fb10cd165e490867f5d61fa366086ea40c73e549a020103dcb4f597e. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 294.36it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 197, 768]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import ViTFeatureExtractor, ViTModel\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"huggingface/cats-image\")\n",
    "image = dataset[\"test\"][\"image\"][0]\n",
    "\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "model = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "\n",
    "inputs = feature_extractor(image, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "last_hidden_states = outputs.last_hidden_state\n",
    "list(last_hidden_states.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.pooler_output.size()[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['pixel_values'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, set_caching_enabled\n",
    "from transformers import (\n",
    "    # Preprocessing / Common\n",
    "    AutoTokenizer, AutoFeatureExtractor,\n",
    "    # Text & Image Models (Now, image transformers like ViTModel, DeiTModel, BEiT can also be loaded using AutoModel)\n",
    "    AutoModel,            \n",
    "    # Training / Evaluation\n",
    "    TrainingArguments, Trainer,\n",
    "    # Misc\n",
    "    logging\n",
    ")\n",
    "# SET CACHE FOR HUGGINGFACE TRANSFORMERS + DATASETS\n",
    "os.environ['HF_HOME'] = os.path.join(\".\", \"cache\")\n",
    "# SET ONLY 1 GPU DEVICE\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "set_caching_enabled(True)\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "NVIDIA A40\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "#     print('Memory Usage:')\n",
    "#     print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "#     print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 ('vqa': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "906903d2a2143b85de940e97fb0e90176e95359f5094a6f7aa7965cc739ab055"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
